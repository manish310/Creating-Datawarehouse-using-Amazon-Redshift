#  Data Warehouse Using Amazon Redshift cluster

### 1. Overview and purpose of the Project:

> In this project, we need to create a Cloud based Data Warehouse using Amazon Redshift cluster and ETL data pipeline using python and SQL to load data from flat files(JSON),which are present at Amazon S3, into Datawarehouse tables. The data is currently lying in form of JSON files. The JSON files contain factual data about songs and user behavioural data of the song playing App(Sparkify). However, data is not of much use in form of JSON file format for analysis point of view. Hence, task is to load the data into Cloud based Data Warehouse tables, so that analytics team can easily acess it and use it for analysis purpose.

### 2. File Structure and Database Design: 

> The data is present in two JSON files. First one, song dataset, contains metadata about songs and artists of the songs. Other one, log dataset, is basically a log file contains user's logs generated by event simulator.
1. Song dataset contains following columns:['artist_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_name', 'duration', 'num_songs', 'song_id', 'title', 'year']
2. Log dataset contains following columns: ['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName','length', 'level', 'location', 'method', 'page', 'registration','sessionId', 'song', 'status', 'ts', 'userAgent', 'userId']

>Based on the available data in the above two files, for database design star schema would be appropriate seeing the simplicity and its wide use in data warehouse design. Using start schema we can design one fact table and four dimension tables as below: 

>##### Fact Table: 
1. songplays (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

>##### Dimesion Table:
2. users (user_id, first_name, last_name, gender, level)
3. songs (song_id, title, artist_id, year, duration)
4. artists (artist_id, name, location, latitude, longitude)
5. time (start_time, hour, day, week, month, year, weekday)

### 3. Python Scripts used and steps to Run them:

>Followings are the files used in this project: 
1. sql_queries.py: This file contains basic implemention of DROP, INSERT, DELETE and SELECT queries, which are used in create_tables.py. This is not a standalone program rather a helping file.
2. create_tables.py: This is a standalone program that we need to run to create Redshift cluster connection, drop old tables and create new ones.This file uses file 'sql_queries.py'. 
3. etl.py: This is also standalone program file. This file contains implementaion of etl pipeline. We need to run this file to load JSON files' data into Redshift cluster tables.
4. quality_checks.ipynb: This one jupyter notebook file to check whether data is loaded in tables correctly or not. 

>##### Steps to run python scripts in terminal: 
1. First of all run the script 'create_tables.py' using command-'python create_tables.py'.
2. Then run the script 'etl.py' using command- 'python etl.py'.
3. Lastly we can check whether data loaded in tables correctly or not using the jupyter notebook file-'quality_checks.ipynb'.
